import sys
sys.path.append("../")
import fire
from scripts.optimize import Optimize
from lolbo.oas_objective import OasObjective
# from lolbo.utils.mol_utils.load_data import load_molecule_train_data, compute_train_zs
import math 
import pandas as pd 
import torch 
from constants import PATH_TO_VAE_STATE_DICT, PATH_TO_INITILIZATION_DATA

class OasOptimization(Optimize):
    """
    Run LOLBO Optimization 
    Args:
        path_to_vae_statedict: Path to state dict of pretrained VAE,
        max_string_length: Limit on string length that can be generated by VAE 
            (without a limit we can run into OOM issues)
    """
    def __init__(
        self,
        path_to_vae_statedict: str=PATH_TO_VAE_STATE_DICT,
        max_string_length: int=1024,
        optimize_pose: bool=False, # allow additional pose optimization after alignment
        min_allowed_edit_dist=-1, # min allowed edit dist from parental h chain seq (constraint, -1 => no constraint)
        **kwargs
    ):
        self.path_to_vae_statedict = path_to_vae_statedict
        self.max_string_length = max_string_length
        self.optimize_pose = optimize_pose
        self.min_allowed_edit_dist = min_allowed_edit_dist

        super().__init__(**kwargs)

        # add args to method args dict to be logged by wandb
        self.method_args['opt1'] = locals()
        del self.method_args['opt1']['self']


    def initialize_objective(self):
        # initialize objective
        self.objective = OasObjective(
            task_id=self.task_id,
            path_to_vae_statedict=self.path_to_vae_statedict,
            max_string_length=self.max_string_length,
            optimize_pose=self.optimize_pose,
            min_allowed_edit_dist=self.min_allowed_edit_dist,
        )
        # if train zs have not been pre-computed for particular vae, compute them 
        #   by passing initialization selfies through vae 
        if self.init_train_z is None:
            self.init_train_z = self.compute_train_zs()
        self.init_train_c = self.objective.compute_constraints(self.init_train_x)

        return self
    
    def compute_train_zs(
        self,
        bsz=64
    ):
        init_zs = []
        # make sure vae is in eval mode 
        self.objective.vae.eval() 
        n_batches = math.ceil(len(self.init_train_x)/bsz)
        for i in range(n_batches):
            xs_batch = self.init_train_x[i*bsz:(i+1)*bsz] 
            zs, _ = self.objective.vae_forward(xs_batch)
            init_zs.append(zs.detach().cpu())
        init_zs = torch.cat(init_zs, dim=0)
        # now save the zs so we don't have to recompute them in the future:
        state_dict_file_type = self.objective.path_to_vae_statedict.split('.')[-1] # usually .pt or .ckpt
        path_to_init_train_zs = self.objective.path_to_vae_statedict.replace(f".{state_dict_file_type}", '-train-zs.csv')
        zs_arr = init_zs.cpu().detach().numpy()
        pd.DataFrame(zs_arr).to_csv(path_to_init_train_zs, header=None, index=None) 

        return init_zs


    def load_train_data(self):
        ''' Load in or randomly initialize self.num_initialization_points
            total initial data points to kick-off optimization 
            Must define the following:
                self.init_train_x (a list of x's)
                self.init_train_y (a tensor of scores/y's)
                self.init_train_z (a tensor of corresponding latent space points)
            '''
        df = pd.read_csv(PATH_TO_INITILIZATION_DATA)
        train_x_seqs = df['seq'].values.tolist()
        train_y = torch.from_numpy(df[self.task_id].values).float() 
        # Here we assume init_data.csv has columsn 'seq', 'dfire', 'dfire2', 'cpydock'
        self.num_initialization_points = min(self.num_initialization_points, len(train_x_seqs))
        self.load_train_z()
        self.init_train_x = train_x_seqs[0:self.num_initialization_points]
        train_y = train_y[0:self.num_initialization_points]
        self.init_train_y = train_y.unsqueeze(-1)
        return self 

    def load_train_z(
        self,
    ):
        state_dict_file_type = self.path_to_vae_statedict.split('.')[-1] # usually .pt or .ckpt
        path_to_init_train_zs = self.path_to_vae_statedict.replace(f".{state_dict_file_type}", '-train-zs.csv')
        # if we have a path to pre-computed train zs for vae, load them
        try:
            zs = pd.read_csv(path_to_init_train_zs, header=None).values
            # make sure we have a sufficient number of saved train zs
            assert len(zs) >= self.num_initialization_points
            zs = zs[0:self.num_initialization_points]
            zs = torch.from_numpy(zs).float()
        # otherwisee, set zs to None 
        except: 
            zs = None 
        self.init_train_z = zs 
        return self 

if __name__ == "__main__":
    fire.Fire(OasOptimization)
